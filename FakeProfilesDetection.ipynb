{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key  = 'd9Ksoz6Wb1jD0mqbW8rjaSNb7'\n",
    "consumer_secret = 'pHXnVSJeLbOxaYlbOR7BWFdDNhZSF6IzegZV87qUSUqy6Qe8qG'\n",
    "access_token = '3648603434-dGRu1nHet22tdoYeqaAGoN8MyZrNw9oXZQvGZUD'\n",
    "access_token_secret = 'PZ8pcQBCb5zVPLRQNVQZc3Yzi0rz1wPef6O7RO7gzcvOf'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names: \n",
      "RyanSAdams\n",
      "peakbengaluru\n",
      "subbu_ananth\n",
      "sandeepnailwal\n",
      "BanklessHQ\n",
      "TyDanielSmith\n",
      "rarible\n",
      "NFTLately\n",
      "khushbooverma_\n",
      "jivraj_sachar\n",
      "upadhyay_harsh1\n",
      "entrackr\n",
      "CoinSwitchKuber\n",
      "_SambhavJain_\n",
      "paulg\n",
      "5aitec\n",
      "dheeraj_sinha\n",
      "sumjain\n",
      "_surgeahead\n",
      "Akshat_World\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'rajshamani').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset0.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names:\n",
      "TheRock\n",
      "michaelstrahan\n",
      "RJScaringe\n",
      "Rivian\n",
      "BarackObama\n",
      "chefjoseandres\n",
      "VanJones68\n",
      "BezosEarthFund\n",
      "DrAndrewSteer\n",
      "BezosFoundation\n",
      "WilliamShatner\n",
      "ajassy\n",
      "washingtonpost\n",
      "LOTRonPrime\n",
      "clubforfuture\n",
      "blueorigin\n",
      "AmazonScience\n",
      "climatepledge\n",
      "alexa99\n",
      "AmazonKindle\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names:')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'JeffBezos').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset1.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names:\n",
      "CHARANJITCHANNI\n",
      "Bhupendrapbjp\n",
      "BSBommai\n",
      "jairamthakurbjp\n",
      "byadavbjp\n",
      "RCP_Singh\n",
      "JM_Scindia\n",
      "AshwiniVaishnaw\n",
      "MeNarayanRane\n",
      "pushkardhami\n",
      "himantabiswa\n",
      "mkstalin\n",
      "TIRATHSRAWAT\n",
      "tarkishorepd\n",
      "renu_bjp\n",
      "AjitPawarSpeaks\n",
      "HemantSorenJMM\n",
      "Dchautala\n",
      "OfficeofUT\n",
      "loksabhaspeaker\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names:')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'PMOIndia').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset2.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names:\n",
      "pcsreeram\n",
      "NiharikaGajula\n",
      "ShivaNirvana\n",
      "sekharkammula\n",
      "narendramodi\n",
      "ActorMadhavan\n",
      "nikethbommi\n",
      "Adityaracing\n",
      "AdiviSesh\n",
      "ssk1122\n",
      "KTRTRS\n",
      "krishnammuthu\n",
      "reyzabahfen\n",
      "IndiaTriumph\n",
      "MaceoPlex\n",
      "ADV1wheels\n",
      "anyasamusic\n",
      "AudemarsPiguet\n",
      "richardbranson\n",
      "elonmusk\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names:')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'chay_akkineni').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset3.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names:\n",
      "sumank\n",
      "PrashantKishor\n",
      "KeerthyOfficial\n",
      "MahuaMoitra\n",
      "MrinalPande1\n",
      "Upadhyay_Cavita\n",
      "fayedsouza\n",
      "amalaakkineni1\n",
      "MunnangiBalu\n",
      "UrbanKisaan\n",
      "iSumanth\n",
      "saakiworld\n",
      "EarlyEkam\n",
      "TeluguBulletin\n",
      "baradwajrangan\n",
      "itisthatis\n",
      "SS_Screens\n",
      "SonyMusicSouth\n",
      "MrAkvarious\n",
      "drrashmishetty\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names:')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'Samanthaprabhu2').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset4.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names: \n",
      "Tyson_Fury\n",
      "officialbantams\n",
      "zainsinterlude\n",
      "inzaynbaby\n",
      "zehlanisroad\n",
      "ZaddyMyLegend\n",
      "zzaynlessly\n",
      "zaynright\n",
      "bestzaynsphotos\n",
      "icarusfy\n",
      "Iucozajn\n",
      "soIemnIyswear\n",
      "vibezual\n",
      "Zeesmagic\n",
      "zjmxlegend\n",
      "captainzarvel\n",
      "fxckilam\n",
      "youknowhooiam\n",
      "sHebyzayn\n",
      "1O237\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'zaynmalik').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset5.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names: \n",
      "ravishndtv\n",
      "Travelunion_TU\n",
      "GroundZ22136605\n",
      "ankurahospitals\n",
      "anuraagmuskaan\n",
      "Ujjwal_Patni\n",
      "salonayyy\n",
      "AslamShaikh_MLA\n",
      "SaraAliKhan\n",
      "ImRaina\n",
      "DrKumarVishwas\n",
      "SonuSoodSena\n",
      "FcSonuSoodMP\n",
      "sonusoodharish\n",
      "tusharchavan\n",
      "vrikshorg\n",
      "PrateikDas\n",
      "SoodFoundation\n",
      "manukumarjain\n",
      "KuberanH\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'SonuSood').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset6.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names: \n",
      "VizeCare\n",
      "Great_Learning\n",
      "one8world\n",
      "cornerstoneway\n",
      "AmitShah\n",
      "IqooInd\n",
      "DangeRussWilson\n",
      "vkfofficial\n",
      "AudiIN\n",
      "ScentialsWorld\n",
      "pumacricket\n",
      "imjadeja\n",
      "HimaDas8\n",
      "OfficialCSFilmz\n",
      "ritikanagpal\n",
      "ChristieLinford\n",
      "narendramodi\n",
      "chetrisunil11\n",
      "Asherzzy\n",
      "bhawnadhingra2\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'imVkohli').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset7.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names: \n",
      "AnushkaQuotes\n",
      "Varun_dvn\n",
      "NushBrand\n",
      "the_asfc\n",
      "AnushkaSharmaFP\n",
      "BladesOf_Glory\n",
      "AfrozShah1\n",
      "JamesBlunt\n",
      "AnimalAid_India\n",
      "thebetterindia\n",
      "NehaalG\n",
      "speakingtree\n",
      "MikaSingh\n",
      "Sethrogen\n",
      "OfficialCSFilmz\n",
      "IndianExpress\n",
      "Variety\n",
      "rizwanahmed\n",
      "prasarbharati\n",
      "anuradhasays\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'AnushkaSharma').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset8.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friends Names: \n",
      "Google\n",
      "rajshamani\n",
      "AmazonScience\n",
      "itsSSR\n",
      "ProfFeynman\n",
      "ThisIsDSP\n",
      "mahika13\n",
      "HillaryClinton\n",
      "lexfridman\n",
      "Tesla\n",
      "isro\n",
      "NASA\n",
      "BarackObama\n",
      "BillGates\n",
      "SpaceX\n",
      "AskAnshul\n",
      "satyanadella\n",
      "sundarpichai\n",
      "elonmusk\n",
      "POTUS45\n"
     ]
    }
   ],
   "source": [
    "print('Friends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'Charishma_Maram').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset9.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frirends Names: \n",
      "VaralaEnosh\n",
      "Tesla\n",
      "BarackObama\n",
      "satyanadella\n",
      "BillGates\n",
      "elonmusk\n",
      "IndianExpress\n",
      "IndiaToday\n",
      "Google\n",
      "nikifyinglife\n",
      "namik_paul\n",
      "urstrulyMahesh\n",
      "upasanakonidela\n",
      "NASA\n",
      "narendramodi\n",
      "SonuSood\n"
     ]
    }
   ],
   "source": [
    "print('Frirends Names: ')\n",
    "friends = []\n",
    "for friend in tweepy.Cursor(api.friends, screen_name = 'RachelNakkala').items(20):\n",
    "    try:\n",
    "                friends.append(friend.screen_name)\n",
    "                print(friend.screen_name)\n",
    "                time.sleep()\n",
    "    except Exception as e:\n",
    "                pass\n",
    "\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset10.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "#extracting tweets from users\n",
    "Total_Data = []\n",
    "fo = open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset9.txt\", \"r\")\n",
    "f = fo.readlines()\n",
    "fo.close()\n",
    "dataset = map(lambda s: s.strip(),f)\n",
    "try:\n",
    "    for datavar in dataset:\n",
    "        data = api.get_user(datavar)\n",
    "        counter = 0\n",
    "        for status in tweepy.Cursor(api.user_timeline, id = datavar).items(30):\n",
    "            try:\n",
    "                counter= counter+1\n",
    "                Total_Data.append(status)\n",
    "                time.sleep()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "except Exception as e:\n",
    "    pass\n",
    "print(len(Total_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 19)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import pandas as pd\n",
    "\n",
    "def process_http(string):\n",
    "    url_count = 0\n",
    "    for i in string.split():\n",
    "        s, n, p, pa, q, f = urllib.parse.urlparse(i)\n",
    "        if s and n:\n",
    "            url_count += 1\n",
    "    return url_count\n",
    "\n",
    "def process_hashtag(string):\n",
    "    hashtag_count = 0\n",
    "    for i in string.split():\n",
    "        s, n, p, pa, q, f = urllib.parse.urlparse(i)\n",
    "        if i[:1] == '#':\n",
    "            hashtag_count += 1\n",
    "    return hashtag_count\n",
    "\n",
    "def process_mention(string):\n",
    "    mention_count=0\n",
    "    for i in string.split():\n",
    "        s, n, p, pa, q, f = urllib.parse.urlparse(i)\n",
    "        if i[:1] == '@':\n",
    "            mention_count += 1\n",
    "    return mention_count\n",
    "\n",
    "def process_data(Total_Data):\n",
    "    TwittID = [tweet.id for tweet in Total_Data]\n",
    "    # Making the dataset in pandas frame\n",
    "    Data = pd.DataFrame(TwittID, columns = ['TwittID'])\n",
    "    # processing the data in Tweet level\n",
    "    Data[\"TextData\"] = [tweet.text for tweet in Total_Data]\n",
    "    Data[\"TweetCreatedAt\"] = [tweet.created_at for tweet in Total_Data]\n",
    "    Data[\"RetweetCount\"] = [tweet.retweet_count for tweet in Total_Data]\n",
    "    Data[\"TweetFavouriteCount\"] = [tweet.favorite_count for tweet in Total_Data]\n",
    "    Data[\"TweetSource\"] = [tweet.source for tweet in Total_Data]\n",
    "    \n",
    "    # processing the data in User Graph level\n",
    "    \n",
    "    Data[\"UserID\"] = [tweet.author.id for tweet in Total_Data]\n",
    "    Data[\"UserScreenName\"] = [tweet.author.screen_name for tweet in Total_Data]\n",
    "    Data[\"UserName\"] = [tweet.author.name for tweet in Total_Data]\n",
    "    Data[\"UserCreatedAt\"] = [tweet.author.created_at for tweet in Total_Data]\n",
    "    Data[\"UserDescription\"] = [tweet.author.description for tweet in Total_Data]\n",
    "    Data[\"UserDescriptionLength\"] = [len(tweet.author.description) for tweet in Total_Data]\n",
    "    Data[\"UserFollowersCount\"] = [tweet.author.followers_count for tweet in Total_Data]\n",
    "    Data[\"UserFriendsCount\"] = [tweet.author.friends_count for tweet in Total_Data]\n",
    "    Data[\"UserLocation\"] = [tweet.author.location for tweet in Total_Data]\n",
    "    \n",
    "    # Data[\"url\"] = [tweet.author.url for in Total_Data]\n",
    "    # Data[\"User_mention\"] = [user_mentions.author.screen_name for tweet in Total_Data]\n",
    "    # Data[\"HashTag\"] = [hashtag.text for tweet in Total_Data]\n",
    "    \n",
    "    Data[\"HttpCount\"] = [process_http(tweet.text) for tweet in Total_Data]\n",
    "    Data[\"HashtagCount\"] = [process_hashtag(tweet.text) for tweet in Total_Data]\n",
    "    Data[\"MentionCount\"] = [process_mention(tweet.text) for tweet in Total_Data]\n",
    "    Data[\"TweetCount\"] = [tweet.author.statuses_count for tweet in Total_Data]\n",
    "    return Data\n",
    "Data = process_data(Total_Data)\n",
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TwittID</th>\n",
       "      <th>TextData</th>\n",
       "      <th>TweetCreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetFavouriteCount</th>\n",
       "      <th>TweetSource</th>\n",
       "      <th>UserID</th>\n",
       "      <th>UserScreenName</th>\n",
       "      <th>UserName</th>\n",
       "      <th>UserCreatedAt</th>\n",
       "      <th>UserDescription</th>\n",
       "      <th>UserDescriptionLength</th>\n",
       "      <th>UserFollowersCount</th>\n",
       "      <th>UserFriendsCount</th>\n",
       "      <th>UserLocation</th>\n",
       "      <th>HttpCount</th>\n",
       "      <th>HashtagCount</th>\n",
       "      <th>MentionCount</th>\n",
       "      <th>TweetCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1319725221478268928</td>\n",
       "      <td>RT @IvankaTrump: Yesterday @realDonaldTrump gr...</td>\n",
       "      <td>2020-10-23 19:39:46</td>\n",
       "      <td>28161</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1316456285995446272</td>\n",
       "      <td>RT @WhiteHouse: We are lifting up citizens of ...</td>\n",
       "      <td>2020-10-14 19:10:11</td>\n",
       "      <td>1973</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1316456242366349317</td>\n",
       "      <td>RT @WhiteHouse: “We will continue our V-shaped...</td>\n",
       "      <td>2020-10-14 19:10:01</td>\n",
       "      <td>1623</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>1316456204948905984</td>\n",
       "      <td>RT @WhiteHouse: \"Under the previous administra...</td>\n",
       "      <td>2020-10-14 19:09:52</td>\n",
       "      <td>1232</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1316456167149895680</td>\n",
       "      <td>RT @WhiteHouse: \"My Administration has approve...</td>\n",
       "      <td>2020-10-14 19:09:43</td>\n",
       "      <td>1137</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1316456127316611072</td>\n",
       "      <td>RT @WhiteHouse: It is time to allow Americans ...</td>\n",
       "      <td>2020-10-14 19:09:33</td>\n",
       "      <td>1719</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1316456088015863809</td>\n",
       "      <td>RT @WhiteHouse: \"Our groundbreaking therapies ...</td>\n",
       "      <td>2020-10-14 19:09:24</td>\n",
       "      <td>975</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1316456052171341824</td>\n",
       "      <td>RT @WhiteHouse: “When the China Virus arrived,...</td>\n",
       "      <td>2020-10-14 19:09:16</td>\n",
       "      <td>1119</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1316444010110685185</td>\n",
       "      <td>RT @WhiteHouse: \"My policies have benefited th...</td>\n",
       "      <td>2020-10-14 18:21:25</td>\n",
       "      <td>1089</td>\n",
       "      <td>0</td>\n",
       "      <td>The White House</td>\n",
       "      <td>822215679726100480</td>\n",
       "      <td>POTUS45</td>\n",
       "      <td>President Trump 45 Archived</td>\n",
       "      <td>2017-01-19 22:54:28</td>\n",
       "      <td>This is an archive of a Trump Administration a...</td>\n",
       "      <td>117</td>\n",
       "      <td>32573024</td>\n",
       "      <td>37</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TwittID                                           TextData  \\\n",
       "591  1319725221478268928  RT @IvankaTrump: Yesterday @realDonaldTrump gr...   \n",
       "592  1316456285995446272  RT @WhiteHouse: We are lifting up citizens of ...   \n",
       "593  1316456242366349317  RT @WhiteHouse: “We will continue our V-shaped...   \n",
       "594  1316456204948905984  RT @WhiteHouse: \"Under the previous administra...   \n",
       "595  1316456167149895680  RT @WhiteHouse: \"My Administration has approve...   \n",
       "596  1316456127316611072  RT @WhiteHouse: It is time to allow Americans ...   \n",
       "597  1316456088015863809  RT @WhiteHouse: \"Our groundbreaking therapies ...   \n",
       "598  1316456052171341824  RT @WhiteHouse: “When the China Virus arrived,...   \n",
       "599  1316444010110685185  RT @WhiteHouse: \"My policies have benefited th...   \n",
       "\n",
       "         TweetCreatedAt  RetweetCount  TweetFavouriteCount  \\\n",
       "591 2020-10-23 19:39:46         28161                    0   \n",
       "592 2020-10-14 19:10:11          1973                    0   \n",
       "593 2020-10-14 19:10:01          1623                    0   \n",
       "594 2020-10-14 19:09:52          1232                    0   \n",
       "595 2020-10-14 19:09:43          1137                    0   \n",
       "596 2020-10-14 19:09:33          1719                    0   \n",
       "597 2020-10-14 19:09:24           975                    0   \n",
       "598 2020-10-14 19:09:16          1119                    0   \n",
       "599 2020-10-14 18:21:25          1089                    0   \n",
       "\n",
       "            TweetSource              UserID UserScreenName  \\\n",
       "591  Twitter for iPhone  822215679726100480        POTUS45   \n",
       "592     The White House  822215679726100480        POTUS45   \n",
       "593     The White House  822215679726100480        POTUS45   \n",
       "594     The White House  822215679726100480        POTUS45   \n",
       "595     The White House  822215679726100480        POTUS45   \n",
       "596     The White House  822215679726100480        POTUS45   \n",
       "597     The White House  822215679726100480        POTUS45   \n",
       "598     The White House  822215679726100480        POTUS45   \n",
       "599     The White House  822215679726100480        POTUS45   \n",
       "\n",
       "                        UserName       UserCreatedAt  \\\n",
       "591  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "592  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "593  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "594  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "595  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "596  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "597  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "598  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "599  President Trump 45 Archived 2017-01-19 22:54:28   \n",
       "\n",
       "                                       UserDescription  UserDescriptionLength  \\\n",
       "591  This is an archive of a Trump Administration a...                    117   \n",
       "592  This is an archive of a Trump Administration a...                    117   \n",
       "593  This is an archive of a Trump Administration a...                    117   \n",
       "594  This is an archive of a Trump Administration a...                    117   \n",
       "595  This is an archive of a Trump Administration a...                    117   \n",
       "596  This is an archive of a Trump Administration a...                    117   \n",
       "597  This is an archive of a Trump Administration a...                    117   \n",
       "598  This is an archive of a Trump Administration a...                    117   \n",
       "599  This is an archive of a Trump Administration a...                    117   \n",
       "\n",
       "     UserFollowersCount  UserFriendsCount UserLocation  HttpCount  \\\n",
       "591            32573024                37                       0   \n",
       "592            32573024                37                       0   \n",
       "593            32573024                37                       1   \n",
       "594            32573024                37                       0   \n",
       "595            32573024                37                       0   \n",
       "596            32573024                37                       0   \n",
       "597            32573024                37                       0   \n",
       "598            32573024                37                       0   \n",
       "599            32573024                37                       0   \n",
       "\n",
       "     HashtagCount  MentionCount  TweetCount  \n",
       "591             0             2       11009  \n",
       "592             0             1       11009  \n",
       "593             0             1       11009  \n",
       "594             0             1       11009  \n",
       "595             0             1       11009  \n",
       "596             0             1       11009  \n",
       "597             0             1       11009  \n",
       "598             0             2       11009  \n",
       "599             0             1       11009  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.tail(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data in csv file\n",
    "import sys\n",
    "Data.to_csv(r'C:\\Users\\maram\\OneDrive\\Documents\\Fdataset.csv',sep=',' , encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlessandroFaro2\n",
      "Toyboy4bbw\n",
      "RyRy11123\n",
      "Shaey171\n",
      "franksee1010\n",
      "jf9156\n",
      "franksee1010\n",
      "karlinhos2456\n",
      "franksee1010\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "franksee1010\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "kikou2407\n",
      "karlinhos2456\n",
      "franksee1010\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "karlinhos2456\n",
      "dirtycouple9288\n",
      "ShaheenShahine\n",
      "HappyTom9\n",
      "Manfred_Fridric\n",
      "HappyTom9\n",
      "Ko74891874Cafer\n",
      "Ko74891874Cafer\n",
      "AllSexy_Milfs\n",
      "Cum4u0911\n",
      "69Dobra\n",
      "BrutustheBear3\n",
      "LFatass2020\n",
      "Vetrii_PNG\n",
      "HqMilf\n",
      "Vetrii_PNG\n",
      "69Dobra\n",
      "AllSexy_Milfs\n",
      "NinoF15\n",
      "hakankutlay41\n",
      "FilthyHotCougar\n",
      "abekaz4\n",
      "IvoryWhiteivory\n",
      "meilleurequeue\n",
      "unknown_qg\n",
      "Paerytopia_NFT\n",
      "Jack_the_German\n",
      "Jack_the_German\n",
      "AllSexy_Milfs\n",
      "Cum4u0911\n",
      "meilleurequeue\n",
      "SweetHorny6\n",
      "IMethsex\n",
      "kikou2407\n",
      "Bound2021\n",
      "whatatime69\n",
      "meilleurequeue\n",
      "Kauane34416882\n",
      "Frenchyman33\n",
      "fishyowo\n",
      "MAURICEFLANNER3\n",
      "Gianni30590565\n",
      "meilleurequeue\n",
      "IkaLeVrai\n",
      "Lamond99014011\n",
      "ModelsCoco\n",
      "BravoStayBussn\n",
      "mfrthms\n",
      "yahir_lopez1\n",
      "itzjustMei14\n",
      "Luckylove6469\n",
      "Gianni30590565\n",
      "VampiorLH\n",
      "leah_Gottis\n",
      "H0228Josh\n",
      "meilleurequeue\n",
      "Proud_Milfs\n",
      "kikou2407\n",
      "bighome1722\n",
      "Maturegilfmilf\n",
      "Maturegilfmilf\n",
      "IanTeare1\n",
      "bigcoced\n",
      "Asian9325\n",
      "laje05\n",
      "Proud_Milfs\n",
      "AllSexy_Milfs\n",
      "tantoffb\n",
      "sulek_p\n",
      "MAURICEFLANNER3\n",
      "ObserverAnimal\n",
      "meilleurequeue\n",
      "GDMessebau\n",
      "russiandaddy2\n",
      "russiandaddy2\n",
      "GDMessebau\n",
      "kikou2407\n",
      "kisses4cougar\n",
      "wygWygV5dnQnYm0\n",
      "Maturegilfmilf\n",
      "tacgms\n",
      "hornygorgeus\n",
      "Maturegilfmilf\n",
      "livyluvbot\n",
      "wetnarnz\n",
      "MAURICEFLANNER3\n"
     ]
    }
   ],
   "source": [
    "friends = []\n",
    "class listener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            tweet = data.split(',\"screen_name\":\"')[1].split('\",\"location')[0]\n",
    "            print(tweet)\n",
    "            friends.append(tweet)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print('failed on data' + str(e))\n",
    "            time.sleep(5)\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "try:\n",
    "    for x in range(1,2):\n",
    "        twitterStream.filter(track=[\"cougar\"])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Key board interuption\")\n",
    "with open(r\"C:\\Users\\maram\\OneDrive\\Documents\\dataset9.txt\", \"w\") as f:\n",
    "     for item in friends:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Data = []\n",
    "fo = open(r\"C:\\Users\\maram\\OneDrive\\Documents\\spam0.txt\", \"r\")\n",
    "f = fo.readlines()\n",
    "fo.close()\n",
    "dataset = map(lambda s: s.strip(),f)\n",
    "try:\n",
    "    for datavar in dataset:\n",
    "        data = api.get_user(datavar)\n",
    "        counter = 0\n",
    "        for status in tweepy.Cursor(api.user_timeline, id = datavar).items(30):\n",
    "            try:\n",
    "                counter= counter+1\n",
    "                Total_Data.append(status)\n",
    "                time.sleep()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "except Exception as e:\n",
    "    pass\n",
    "print(len(Total_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import pandas as pd\n",
    "\n",
    "def process_http(string):\n",
    "    url_count = 0\n",
    "    for i in string.split():\n",
    "        s, n, p, pa, q, f = urllib.parse.urlparse(i)\n",
    "        if s and n:\n",
    "            url_count += 1\n",
    "    return url_count\n",
    "\n",
    "def process_hashtag(string):\n",
    "    hashtag_count = 0\n",
    "    for i in string.split():\n",
    "        s, n, p, pa, q, f = urllib.parse.urlparse(i)\n",
    "        if i[:1] == '#':\n",
    "            hashtag_count += 1\n",
    "    return hashtag_count\n",
    "\n",
    "def process_mention(string):\n",
    "    mention_count=0\n",
    "    for i in string.split():\n",
    "        s, n, p, pa, q, f = urllib.parse.urlparse(i)\n",
    "        if i[:1] == '@':\n",
    "            mention_count += 1\n",
    "    return mention_count\n",
    "\n",
    "def process_data(Total_Data):\n",
    "    TwittID = [tweet.id for tweet in Total_Data]\n",
    "    # Making the dataset in pandas frame\n",
    "    Data = pd.DataFrame(TwittID, columns = ['TwittID'])\n",
    "    # processing the data in Tweet level\n",
    "    \n",
    "    Data[\"TextData\"] = [tweet.text for tweet in Total_Data]\n",
    "    Data[\"TweetCreatedAt\"] = [tweet.created_at for tweet in Total_Data]\n",
    "    Data[\"RetweetCount\"] = [tweet.retweet_count for tweet in Total_Data]\n",
    "    Data[\"TweetFavouriteCount\"] = [tweet.favorite_count for tweet in Total_Data]\n",
    "    Data[\"TweetSource\"] = [tweet.source for tweet in Total_Data]\n",
    "    \n",
    "    # processing the data in User Graph level\n",
    "    \n",
    "    Data[\"UserID\"] = [tweet.author.id for tweet in Total_Data]\n",
    "    Data[\"UserScreenName\"] = [tweet.author.screen_name for tweet in Total_Data]\n",
    "    Data[\"UserName\"] = [tweet.author.name for tweet in Total_Data]\n",
    "    Data[\"UserCreatedAt\"] = [tweet.author.created_at for tweet in Total_Data]\n",
    "    Data[\"UserDescription\"] = [tweet.author.description for tweet in Total_Data]\n",
    "    Data[\"UserDescriptionLength\"] = [len(tweet.author.description) for tweet in Total_Data]\n",
    "    Data[\"UserFollowersCount\"] = [tweet.author.followers_count for tweet in Total_Data]\n",
    "    \n",
    "    # processing the data in User Graph level\n",
    "    \n",
    "    Data[\"UserID\"] = [tweet.author.id for tweet in Total_Data]\n",
    "    Data[\"UserScreenName\"] = [tweet.author.screen_name for tweet in Total_Data]\n",
    "    Data[\"UserName\"] = [tweet.author.name for tweet in Total_Data]\n",
    "    Data[\"UserCreatedAt\"] = [tweet.author.created_at for tweet in Total_Data]\n",
    "    Data[\"UserDescription\"] = [tweet.author.description for tweet in Total_Data]\n",
    "    Data[\"UserDescriptionLength\"] = [len(tweet.author.description) for tweet in Total_Data]\n",
    "    Data[\"UserFollowersCount\"] = [tweet.author.followers_count for tweet in Total_Data]\n",
    "    Data[\"UserFriendsCount\"] = [tweet.author.friends_count for tweet in Total_Data]\n",
    "    Data[\"UserLocation\"] = [tweet.author.location for tweet in Total_Data]\n",
    "    \n",
    "    # Data[\"url\"] = [tweet.author.url for in Total_Data]\n",
    "    # Data[\"User_mention\"] = [user_mentions.author.screen_name for tweet in Total_Data]\n",
    "    # Data[\"HashTag\"] = [hashtag.text for tweet in Total_Data]\n",
    "    \n",
    "    Data[\"HttpCount\"] = [process_http(tweet.text) for tweet in Total_Data]\n",
    "    Data[\"HashtagCount\"] = [process_hashtag(tweet.text) for tweet in Total_Data]\n",
    "    Data[\"MentionCount\"] = [process_mention(tweet.text) for tweet in Total_Data]\n",
    "    Data[\"TweetCount\"] = [tweet.author.statuses_count for tweet in Total_Data]\n",
    "    return Data\n",
    "Data = process_data(Total_Data)\n",
    "Data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging all data\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "# get data file names\n",
    "path = 'C:\\Users\\maram\\OneDrive\\Documents\\CompleteDataSet.csv'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "content = []\n",
    "for filename in filenames:\n",
    "    content.append(pd.read_csv(filename, error_bad_lines=False))\n",
    "\n",
    "Total_leg = pd.concat(content, ignore_index=True)\n",
    "Total_leg.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_leg.to_csv('C:\\Users\\maram\\OneDrive\\Documents\\LegitimateDataSet.csv', sep=',' , encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Spammer Data\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "# get data file names\n",
    "path = 'C:\\Users\\maram\\OneDrive\\Documents\\SpammerDataSet.csv'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "content = []\n",
    "for filename in filenames:\n",
    "    content.append(pd.read_csv(filename, error_bad_lines=False))\n",
    "\n",
    "Total_leg = pd.concat(content, ignore_index=True)\n",
    "Total_leg.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_leg.to_csv('C:\\Users\\maram\\OneDrive\\Documents\\SpammerDataSet.csv', sep=',' , encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Total_leg_data = pd.read_csv('LegitimateDataSet.csv')\n",
    "Total_leg_data.fillna(0, inplace=True)\n",
    "Total_leg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_data = Total_leg_data['UserLocation'].value_counts()\n",
    "location_data[2:15].plot(kind='bar', figsize=(14,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18,4)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "text = Total_leg_data['TextData']\n",
    "is_sex = text.str.contains('sex')\n",
    "is_sex=is_sex.astype(float)\n",
    "is_sex.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_leg_data=Total_leg_data.fillna(0)\n",
    "Total_leg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = Total_leg_data[[\"UserFollowersCount\"]]\n",
    "temp1.to_csv('temp1.csv', sep=',',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_leg_data[['RetweetCount']] = Total_leg_data[['RetweetCount']].astype(float)\n",
    "Total_leg_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_leg_data = Total_leg_data[Total_leg_data.TweetCount!=0]\n",
    "len(Total_leg_data[Total_leg_data.TweetCount<30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_leg_data.loc[:,\"AvgHashtag\"] = (Total_leg_data.groupby('UserID')[\"HashtagCount\"].transform('sum'))/30\n",
    "Total_leg_data.loc[:,\"AvgURLCount\"] = (Total_leg_data.groupby('UserID')[\"HttpCount\"].transform('sum'))/30\n",
    "Total_leg_data.loc[:,\"AvgMention\"] = (Total_leg_data.groupby('UserID')[\"MentionCount\"].transform('sum'))/30\n",
    "Total_leg_data.loc[:,\"AvgRetweet\"] = (Total_leg_data.groupby('UserID')[\"RetweetCount\"].transform('sum'))/30\n",
    "Total_leg_data.loc[:,\"AvgFavCount\"] = (Total_leg_data.groupby('UserID')[\"TweetFavouriteCount\"].transform('sum'))/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
